{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1bd0ce",
   "metadata": {},
   "source": [
    "# Text Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b59ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from detection_utils import craft_utils\n",
    "from detection_utils import imgproc\n",
    "from detection_utils import file_utils\n",
    "import json\n",
    "import zipfile\n",
    "from detection_utils.craft import CRAFT\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c45c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fcf1315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from ai_models/craft_mlt_25k.pth\n",
      "\n",
      "Text detection completed in : 2.63s\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"trained_model\": \"ai_models/craft_mlt_25k.pth\",\n",
    "    \"text_threshold\": 0.7,\n",
    "    \"low_text\": 0.4,\n",
    "    \"link_threshold\": 0.4,\n",
    "    \"cuda\": False,\n",
    "    \"canvas_size\": 1280,\n",
    "    \"mag_ratio\": 1.5,\n",
    "    \"poly\": False,\n",
    "    \"show_time\": False,\n",
    "    \"test_folder\": \"./input_frames/\",\n",
    "    \"refine\": False,\n",
    "    \"refiner_model\": \"weights/craft_refiner_CTW1500.pth\"\n",
    "}\n",
    "\n",
    "\n",
    "def copyStateDict(state_dict):\n",
    "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = \".\".join(k.split(\".\")[start_idx:])\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "\n",
    "##---------------------- inference Function ----------------------\n",
    "\n",
    "def test_net(net, image, text_threshold, link_threshold, low_text, cuda, poly, refine_net=None):\n",
    "    t0 = time.time()\n",
    "    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(\n",
    "        image, config[\"canvas_size\"], interpolation=cv2.INTER_LINEAR, mag_ratio=config[\"mag_ratio\"]\n",
    "    )\n",
    "    ratio_h = ratio_w = 1 / target_ratio\n",
    "\n",
    "    x = imgproc.normalizeMeanVariance(img_resized)\n",
    "    x = torch.from_numpy(x).permute(2, 0, 1)\n",
    "    x = Variable(x.unsqueeze(0))\n",
    "    if cuda:\n",
    "        x = x.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y, feature = net(x)\n",
    "\n",
    "    score_text = y[0, :, :, 0].cpu().data.numpy()\n",
    "    score_link = y[0, :, :, 1].cpu().data.numpy()\n",
    "\n",
    "    if refine_net is not None:\n",
    "        with torch.no_grad():\n",
    "            y_refiner = refine_net(y, feature)\n",
    "        score_link = y_refiner[0, :, :, 0].cpu().data.numpy()\n",
    "\n",
    "    t0 = time.time() - t0\n",
    "    t1 = time.time()\n",
    "\n",
    "    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly)\n",
    "    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "    polys = craft_utils.adjustResultCoordinates(polys, ratio_w, ratio_h)\n",
    "    for k in range(len(polys)):\n",
    "        if polys[k] is None:\n",
    "            polys[k] = boxes[k]\n",
    "\n",
    "    t1 = time.time() - t1\n",
    "\n",
    "    render_img = np.hstack((score_text.copy(), score_link))\n",
    "    ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n",
    "\n",
    "    if config[\"show_time\"]:\n",
    "        print(\"\\ninfer/postproc time : {:.3f}/{:.3f}\".format(t0, t1))\n",
    "\n",
    "    return boxes, polys, ret_score_text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result_folder = './result/'\n",
    "    os.makedirs(result_folder, exist_ok=True)\n",
    "    image_list, _, _ = file_utils.get_files(config[\"test_folder\"])\n",
    "\n",
    "    net = CRAFT()\n",
    "    print('Loading weights from ' + config[\"trained_model\"] )\n",
    "    if config[\"cuda\"]:\n",
    "        net.load_state_dict(copyStateDict(torch.load(config[\"trained_model\"])))\n",
    "    else:\n",
    "        net.load_state_dict(copyStateDict(torch.load(config[\"trained_model\"], map_location='cpu')))\n",
    "\n",
    "    if config[\"cuda\"]:\n",
    "        net = net.cuda()\n",
    "        net = nn.DataParallel(net)\n",
    "        cudnn.benchmark = False\n",
    "\n",
    "    net.eval()\n",
    "    refine_net = None\n",
    "    t = time.time()\n",
    "    for k, image_path in enumerate(image_list):\n",
    "        \n",
    "        image = imgproc.loadImage(image_path) #-- load image\n",
    "        ##--- run detection\n",
    "        boxes, polys, score_text = test_net(net, image, config[\"text_threshold\"], config[\"link_threshold\"], config[\"low_text\"], config[\"cuda\"], config[\"poly\"], refine_net)\n",
    "\n",
    "        filename, file_ext = os.path.splitext(os.path.basename(image_path))\n",
    "\n",
    "        ##---- Save cropped regions \n",
    "        crop_folder = os.path.join(result_folder, \"crops\")\n",
    "        os.makedirs(crop_folder, exist_ok=True) ##--- result/crops\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            rect = np.array(box).astype(np.int32)\n",
    "            x, y, w, h = cv2.boundingRect(rect)\n",
    "            crop = image[y:y+h, x:x+w]\n",
    "\n",
    "            crop_file = os.path.join(crop_folder, f\"{filename}_box_{i+1}.jpg\")\n",
    "            cv2.imwrite(crop_file, crop)\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"\\nText detection completed in : {:.2f}s\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583e15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fd2f0d7",
   "metadata": {},
   "source": [
    "# Text recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3724da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from recognition_utils.utils import CTCLabelConverter, AttnLabelConverter\n",
    "from recognition_utils.dataset import RawDataset, AlignCollate\n",
    "from recognition_utils.model import Model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c816d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ai_models/best_accuracy.pth\n",
      "--------------------------------------------------------------------------------\n",
      "image_path               \tpredicted_labels         \tconfidence score\n",
      "--------------------------------------------------------------------------------\n",
      "result/crops\\image_validation_94_box_1.jpg\t053660                   \t0.9693\n",
      "result/crops\\image_validation_939_box_1.jpg\t479643                   \t0.4904\n",
      "result/crops\\image_validation_941_box_1.jpg\t480149                   \t0.4348\n",
      "result/crops\\image_validation_943_box_1.jpg\t481096                   \t0.9263\n",
      "result/crops\\image_validation_944_box_1.jpg\t481867                   \t0.8861\n",
      "result/crops\\image_validation_945_box_1.jpg\t482198                   \t0.9726\n",
      "result/crops\\image_validation_949_box_1.jpg\t483540                   \t0.6824\n",
      "result/crops\\image_validation_988_box_1.jpg\t501703                   \t0.9941\n",
      "result/crops\\image_validation_991_box_1.jpg\t502942                   \t0.9896\n",
      "result/crops\\image_validation_992_box_1.jpg\t503914                   \t0.7862\n",
      "result/crops\\image_validation_998_box_1.jpg\t506005                   \t0.9739\n",
      "result/crops\\image_validation_999_box_1.jpg\t506764                   \t0.9729\n"
     ]
    }
   ],
   "source": [
    "class Opt:\n",
    "    \n",
    "    ##------- input_frams/model settings\n",
    "    image_folder = 'result/crops'\n",
    "    saved_model = 'ai_models/best_accuracy.pth'\n",
    "    \n",
    "    ##------ Model architecture\n",
    "    Transformation = 'TPS'\n",
    "    FeatureExtraction = 'ResNet'\n",
    "    SequenceModeling = 'BiLSTM'\n",
    "    Prediction = 'CTC'#'Attn'\n",
    "\n",
    "    ##-----Input/output settings\n",
    "    imgH = 32\n",
    "    imgW = 100\n",
    "    rgb = False\n",
    "    character = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "    sensitive = False  \n",
    "    PAD = False\n",
    "\n",
    "    ##----- Training/inference settings\n",
    "    batch_max_length = 25\n",
    "    num_fiducial = 20\n",
    "    input_channel = 1\n",
    "    output_channel = 512\n",
    "    hidden_size = 256\n",
    "    batch_size = 192\n",
    "    workers = 0\n",
    "\n",
    "    ##---- Device settings\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "\n",
    "opt = Opt()\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "\n",
    "##------------------- recognition block -------------------\n",
    "\n",
    "def demo(opt):\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "\n",
    "    model = Model(opt)\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "    print('Loading model from %s' % opt.saved_model)\n",
    "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
    "\n",
    "    AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "    demo_data = RawDataset(root=opt.image_folder, opt=opt)\n",
    "    demo_loader = torch.utils.data.DataLoader(\n",
    "        demo_data, batch_size=opt.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=opt.workers,\n",
    "        collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_tensors, image_path_list in demo_loader:\n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "            length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text_for_pred)\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_str = converter.decode(preds_index, preds_size)\n",
    "            else:\n",
    "                preds = model(image, text_for_pred, is_train=False)\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_str = converter.decode(preds_index, length_for_pred)\n",
    "\n",
    "            log = open('./result/recognition_result.txt', 'a')\n",
    "            dashed_line = '-' * 80\n",
    "            head = f'{\"image_path\":25s}\\t{\"predicted_labels\":25s}\\tconfidence score'\n",
    "            print(f'{dashed_line}\\n{head}\\n{dashed_line}')\n",
    "            log.write(f'{dashed_line}\\n{head}\\n{dashed_line}\\n')\n",
    "\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "            for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
    "                if 'Attn' in opt.Prediction:\n",
    "                    pred_EOS = pred.find('[s]')\n",
    "                    pred = pred[:pred_EOS]\n",
    "                    pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "                print(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}')\n",
    "                log.write(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}\\n')\n",
    "            log.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b16d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:image_env]",
   "language": "python",
   "name": "conda-env-image_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
